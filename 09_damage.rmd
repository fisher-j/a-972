---
title: Bear and other damage
bibliography: ["./reference/a972.bib"]
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
```

```{r}
require(tidyverse)
library(performance)
```

# Bear damage

## data cleaning

I'll only consider live trees.

```{r}
bd <- d_l %>%
  filter(live) %>%
  mutate(year = factor(year, ordered = FALSE)) %>%
  select(-c(h_dist, azi, x, y, cc, live, ht))
```

There are trees with "healed over" in the notes, most of these are in 2008. It makes sense if these trees are subsequently listed as not bear damaged. 

I'm assuming that any tree that goes from bear damaged in 2008 to not bear damaged in 2013 is in fact healed and that any damage in 2018 is new damage.

```{r}

# # Use this to look at any "healed" trees
# bd %>% 
#   group_by(tree_id) %>%
#   filter(any(str_detect(tolower(notes), "healed"))) %>%
#   color_groups()

```

In looking at notes, trees trees that have "old bear damage" recorded are treated inconsistently, some are recorded with bear damage, some without. I'll assume that trees recorded as not bear damaged are either undamaged or completely healed, and subsequent damage implies a new bear incidence of bear damage.

```{r}

# # Use this to look at any "old bd" trees
# bd %>% 
#   group_by(tree_id) %>%
#   filter(any(str_detect(tolower(notes), "old"))) %>%
#   color_groups()

```

Are there trees that are recorded as bear damaged in one period and then not in the next period? Put another way, are bear damaged trees dropped from the list for one reason or another? 

There are 64 trees that are dropped (all in 2013), of these, 14 are subsequently listed as damaged (all in 2018). As stated above, I will consider these valid occurrences of new damage.

```{r}
bear_dropped <- bd %>%
  group_by(tree_id) %>%
  mutate(bear_dropped = lag(bear) & !bear) %>%
  filter(any(bear_dropped)) %>%
  mutate(id = cur_group_id()) %>%
  relocate(id) %>%
  arrange(id)

# # all dropped bear damage trees occur in 2013
# filter(bear_dropped, bear_dropped) %>% pull(year) %>% unique()

# color_groups(bear_dropped)

# # trees that are re-attacked
# bear_dropped %>%
#   filter(any(bear & !lag(bear))) %>%
#   mutate(id = cur_group_id(), .before = 1) %>%
#   color_groups()

```

I need to create another variable which indicates if the damage is new for that period, when a trees goes from undamaged to damaged. I will also count trees as new bear damage when the damage increases from one period to the next ie. when condition code increases from 17 or 18 to 19 or 20.

I'll also add a variable indicating whether a tree was damaged in 2013

```{r}

# # this was used for ensuring all bear damage stuck with a tree throughout its life
# # I've since decided to allow trees to "completely heal," as the data seems to suggest this
# cum_logic <- function(x) {
#   if (any(x)) {
#     idx <- min(which(x))
#     x[idx:length(x)] <- TRUE
#   }
#   return(x)
# }

bd <- bd %>%
  group_by(tree_id) %>%
  mutate(
    bear_mag = as.numeric(get_cond(17, 18, 19, str = TRUE)),
    bear_new = bear & year %in% c("init", "08") | bear & !lag(bear) | bear_mag > lag(bear_mag),
    bear_new = if_else(is.na(bear_new), FALSE, bear_new)
    ) %>%
  select(-bear_mag) %>%
  ungroup()

```

I'm going to drop alder and hemlock, because they are never attacked by bear. I am also dropping spruce in order to be consistent with the rest of my analysis. 

There were about 30 spruce trees before thinning and about half of these had bear damage. In 2013, there were 24 spruce and 4 of them received damage.

```{r}

bd <- filter(bd, spp %in% c("SESE3", "PSMEM")) %>%
  mutate(spp2 = spp)

```

## Summary

Here is percent new bear damage over time. The H40 and L40 treatments seem to have the largest increases.

In the H40 treatment there is a decline in percent new bear damage in two plots, whereas for the L40 treatment, only one declines, and it is anomalous in that is the only plot that sees an increase from 2008 to 2013.

Error bars represent 1 SE of mean

```{r}

bear_plot <- function(data, var) {
  my_dodge = position_dodge(width = 0.5)
  ggplot(data, aes(year, {{var}}, color = treatment, group = treatment)) + 
    geom_line(position = my_dodge) +
    geom_point(position = my_dodge) +
    geom_errorbar(aes(ymin = {{var}} - se, ymax = {{var}} + se), width = 0.2, position = my_dodge)
}

bd %>%
  # filter(year != "init") %>%
  group_by(year, treatment, plot) %>% 
  summarize(pct_bear = sum(bear_new, na.rm = TRUE) / n()) %>% 
  summarize(avg_pct_bear = mean(pct_bear), se = sd(pct_bear) / sqrt(n()) ) %>% 
  bear_plot(avg_pct_bear) +
    labs(title = "Average percent new bear damage for each treatment ±SE")


bd %>%
  # filter(year != "init") %>%
  group_by(year, treatment, plot) %>% 
  summarize(cnt_bear = sum(bear_new, na.rm = TRUE)) %>% 
  summarize( avg_cnt_bear = mean(cnt_bear), se = sd(cnt_bear) / sqrt(n()) ) %>% 
  bear_plot(avg_cnt_bear) +
    labs(title = "Average count new bear damage for each treatment ±SE")

bd %>%
  filter(year != "init") %>%
  group_by(year, treatment, plot) %>% 
  summarize(pct_bear = sum(bear_new, na.rm = TRUE) / n()) %>% 
  # summarize(avg_pct_bear = mean(pct_bear)) %>% 
  ggplot(aes(year, pct_bear, color = treatment, group = plot)) + 
    geom_line(position = position_dodge(width = 0.4), size = 1, alpha = 0.6) +
    facet_wrap(~ treatment) +
    theme(legend.position = "none") +
    geom_point(position = position_dodge(width = 0.4)) +
    scale_x_discrete(expand = expansion(mult = 0.2)) +
    labs(title = "Actual percent new bear damage for each treatment and plot")


bd %>%
  filter(year != "init") %>%
  group_by(treatment, year, spp2) %>%
  summarize(bear_new = sum(bear_new)) %>%
  ungroup() %>%
  ggplot(aes(treatment, bear_new, fill = spp2)) + 
    geom_col(position = "stack", color = "black") +
    facet_wrap(~ year) +
    scale_fill_manual(values = palette()) +
    labs(
      title = "Total sum of new bear damage for each treatment/species",
      y = "Count of bear damage",
      x = "Treatment",
      fill = NULL
    )


bd %>%
  group_by(treatment, year, plot) %>%
  summarize(bear_new = sum(bear_new)) %>%
  ggplot(aes(year, bear_new, group = str_extract(plot, "\\d"))) + 
    geom_point(position = position_dodge(0.3), alpha = 0.5) +
    geom_line(position = position_dodge(0.3), size = 1, alpha = 0.5) +
    facet_wrap(~ treatment) +
    labs(
      title = "Actual count new bear damage for each plot in treatment",
      y = "Count of bear damage", x = "Year"  
    )

```

## Modeling

There are several potential to modeling this data.

1. Probability of bear damage could be modeled as binary data with a generalized linear model binomial regression with logit link (logistic regression). This, I think would be answering: for a random (average?) tree from a given treatment, what is the probability that it would be bear damaged? I'm not sure we have sufficient observations of damaged trees characterize the distribution. In the case of prediction, we may need to make adjustment for the [imbalance of response](https://stats.stackexchange.com/questions/6067/does-an-unbalanced-sample-matter-when-doing-logistic-regression)

2. Another approach is modeling *percent bear damage* at the plot level. Here linear regression may work, but theoretically, our response is bound by (0, 1). One recommendation here is [Beta regression](https://hansjoerg.me/2019/05/10/regression-modeling-with-proportion-data-part-1/).

3. Also at the plot level, we could model counts using Poisson or negative binomial GLMM. This would answer the question: how many trees can we expect to be bear damaged given a treatment. Here it would probably be important to account for differences between treatments like diameter increment and tree size.

## Logistic regression

We are modeling occurrence of new bear damage in 2013 and 2018. New bear damage in 2008 is not really comparable as it represents accumulated damage over an unspecified amount of time prior to treatment.

My first model is additive and includes `treatment`, `year`, `d_inc2`, `spp2` and random slopes for `plot`, *and* `tree_id`.

The random effects for `tree_id` are very small, these might end up getting dropped.


```{r, warning=TRUE}

bdmd <- subset(bd, year %in% c("13", "18"))

bm1 <- glmer(
  bear_new ~ treatment + year + d_inc2 + spp2 + (1 | plot) + (1 | tree_id),
  family = binomial,
  data = bdmd
  )

summary(bm1)

```

check `allFit` output to see if this model can be considered reliable: The estimated coefficients are very congruent, I think we can trust this model.

```{r}

bm1.all <- allFit(bm1)

summary(bm1.all)$fixef

```

This model seems to be working. Now I'll do more model selection, testing for interactions. All of these models will include `plot` as the only random effect. I compare performance metrics using the R package: performance [@ludeckePerformancePackageAssessment2021]


```{r, warning=TRUE, cache=TRUE}

fl <- list(
    bear_new ~ treatment + year + spp2 + d_inc2
  , bear_new ~ treatment + year + spp2 + scale(ba_inc2)
  , bear_new ~ treatment + year + spp2 + d_inc2 + year:spp2
  , bear_new ~ treatment + year + spp2 + scale(ba_inc2) + year:spp2
  , bear_new ~ treatment + year + spp2 + d_inc2 + treatment:spp2
  , bear_new ~ treatment + year + spp2 + scale(ba_inc2) + treatment:spp2
  , bear_new ~ treatment + year + spp2 + d_inc2 + treatment:year
  , bear_new ~ treatment + year + spp2 + scale(ba_inc2) + treatment:year
  , bear_new ~ treatment + year + spp2 + d_inc2 + spp2:d_inc2
  , bear_new ~ treatment + year + spp2 + scale(ba_inc2) + spp2:scale(ba_inc2)
  , bear_new ~ treatment + year + spp2 + scale(ba_inc2) + year:spp2 + spp2:scale(ba_inc2)
  , bear_new ~ treatment + year + spp2 + d_inc2 + year:spp2 + d_inc2:spp2
)

names(fl) <- seq_along(fl)

make_glmm_mods <- function(dat, fl, w_tree = FALSE){
  ran <- "~ . + (1 | plot)"
  if (w_tree) ran <- paste(ran, "+ (1 | tree_id)")
  eval(bquote(
    lapply(fl, \(x) {
      form <- update(x, ran)
      print(paste("Evaluating: ", deparse1(x)))
      glmer(
        form,
        family = binomial(),
        data = .(substitute(dat)),
        control = glmerControl(optimizer = "bobyqa", optCtrl = list(maxfun=2e7))
      )
    })
  ))
}

make_bglmm_mods <- function(dat, fl, w_tree = FALSE){
  ran <- "~ . + (1 | plot)"
  if (w_tree) ran <- paste(ran, "+ (1 | tree_id)")
  out <- eval(bquote(
    lapply(fl, function(x) {
      mod0 <- glm(x, data = dat)
      print(paste("Evaluating: ", deparse1(x)))
      n_coef <<- length(coef(mod0))
      form <- update(x, ran)
      blme::bglmer(
        form,
        family = binomial(),
        data = .(substitute(dat)),
        fixef.prior = normal(cov = diag(9, n_coef)),
        control = glmerControl(optimizer="bobyqa", optCtrl = list(maxfun=2e7))
      )
    })
  ))
  rm(n_coef, pos = .GlobalEnv)
  return(out)
}

```

Here is the list of models that I am going to test. They include different interaction terms. In addition, I can include `tree_id` as a random effect or not. Preliminary trials, showed that models were unstable when including this term. Within subject (`tree_id`) variance is perfectly correlated with outcome because most trees that are damaged are only damaged once. This seems to show up in the model as with `tree_id` absorbing most of the variance.

There are also problems with complete separation, which I attempt to fix by using a bayesian framework. I tried using penalized regression with a fixed effects only glm (bias reduction). I also tried a bayesian model and specified a [prior variance for the fixed effects as a gaussian distribution with a sd of 3](https://bbolker.github.io/mixedmodels-misc/glmmFAQ.html#penalizationhandling-complete-separation). Ben Bolker [says](https://bbolker.github.io/mixedmodels-misc/ecostats_chap.html#digression-complete-separation) "We can use `bglmer` from the `blme` package to impose zero-mean Normal priors on the fixed effects" 

```{r}
# model list
data.frame(form = sapply(fl, deparse1)) %>% 
  kbl2(row.names = TRUE,
  caption = "Formulas testing different tree-growth variables and intereactions")
```

```{r, cache=TRUE}

# run models
bdm1 <- make_glmm_mods(bdmd, fl)
bdm1_tree <- make_glmm_mods(bdmd, fl, w_tree = TRUE)

bbdm <- make_bglmm_mods(bdmd, fl)
bbdm_tree <- make_bglmm_mods(bdmd, fl, w_tree = TRUE)

```

When looking at the summaries of the models that include `tree_id`, they all fail to converge. I think there is a problem with the fact that within subject variance is perfectly correlated with our response, because trees can, in general only be damaged in one period or the other, trees that receive damage have very high variance, while those that do not have very low variance.

My best guess is that it doesn't make sense to include `tree_id` because of its bimodal error.

Another option could be ensuring that a tree is only damaged in one period, but here are only 3 trees that receive "new" damage in both periods. I doubt that these are having a huge effect. For the most part, trees are only damaged in one period or the other.

```{r}
examine_bdmod <- function(mod, cap) {
  mod_list <- do.call(compare_performance, c(mod, metrics = "common")) %>%
    arrange(AIC)
  to_compare <- as.numeric(c(mod_list$Name[1:4], "2"))
  print(
    mod_list %>%
    kbl2(digits = 3, caption = cap)
  )
  print(
    sjPlot::tab_model(
    mod[to_compare],
    show.ci = FALSE,
    show.aic = TRUE,
    dv.labels = paste("Formula", to_compare),
    title = cap
    )
  )
}

examine_bdmod(bdm1, "GLMM models with random plot only")
examine_bdmod(bdm1_tree, "GLMM models with random plot and tree_id")
examine_bdmod(bbdm, "Bayesian GLMM models with random plot only")
examine_bdmod(bbdm_tree, "Bayesian GLMM models with random plot and tree_id")

```


I need a function for comparing models visually, with their predictions. I think that emmeans does this, but because I can't seem to be sure what I am predicting with emmeans, I will construct my own dataset to make predictions in order to clarify what emmeans are predicting.

```{r}

pred_bd <- function(mod) {
  ba_incs <- seq(
    quantile(bdmd$ba_inc2, 0.01),
    quantile(bdmd$ba_inc2, 0.99),
    length.out = 25
  )
  prediction_data <- expand.grid(
    plot = NA,
    spp2 = c("PSMEM", "SESE3"),
    year = c("13", "18"),
    treatment = c("C", "H40", "H80", "L40", "L80"),
    ba_inc2 = ba_incs
  )
  if (inherits(mod, "merMod")) {
    prediction_data <- cbind(
      prediction_data,
      pred = predict(mod, newdata = prediction_data, re.form = NA, type = "response")
    )
  } else if (inherits(mod, "glm")) {
    prediction_data <- cbind(
      prediction_data,
      pred = predict(mod, newdata = prediction_data, re.form = NA, type = "response")
    )
  }
  prediction_data %>%
    ggplot(aes(ba_inc2, pred, color = treatment)) +
    geom_line() +
    facet_grid(spp2 ~ year)
}

pred_bd(bbdm[[4]])

```

Here I am able to show that the predictions from emmeans are exactly the same as those I am getting from predicting using my own function. 

```{r}

ba_incs <- list(
    ba_inc2 = seq(
      quantile(bdmd$ba_inc2, 0.01),
      quantile(bdmd$ba_inc2, 0.99),
      length.out = 25
    )
  )

emmip(bbdm[[4]],
  treatment ~ ba_inc2 | spp2 + year,
  at = ba_incs,
  type = "response",
  CIs = TRUE
) 

```

## Emmeans: interpreting model

Redwood is clearly more targeted.

```{r}

emmeans(bbdm[[4]], pairwise ~ spp2, type = "response")

```

Here we can see explicit comparisons between treatments

```{r}

mean_treat <- emmeans(bbdm[[4]], ~ treatment, type = "response")

pairs(mean_treat)

pwpp(b2_mean)

b2_mean %>% multcomp::cld(reversed = TRUE)

```

Finally, here is the expected response over levels of diameter increment.

```{r}

emmip(bm2, treatment ~ d_inc2, at = list(d_inc2 = seq(-0.5, 2.8, 0.1)), type = "response")

```



## Model validation

I can check for over dispersion, and [I don't think I need to worry about underdispersion](https://stats.stackexchange.com/questions/568407/how-to-correct-underdispersion-in-logistic-regression)

```{r}


```